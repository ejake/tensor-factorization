{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TensorFlow: An Introduction *\n",
    "====================\n",
    "![MindLABLogo](images/mindlab-logo-simple.png \"MindLab\") \n",
    "![UNLogo](images/unal-logo.png \"MindLab\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** This notebook is heavily based on the TensorFlow documentation, particularly: **\n",
    "    \n",
    "   * https://www.tensorflow.org/versions/master/get_started/basic_usage.html \n",
    "   * https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html \n",
    "   * https://www.tensorflow.org/api_docs\n",
    "   * https://www.tensorflow.org/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>To use TensorFlow we need to understand how TensorFlow:</h3>\n",
    "* Represents computations as graphs (Theano alike)\n",
    "* Executes graphs in the context of Sessions\n",
    "* Represents data as tensors\n",
    "* Maintains state with Variables\n",
    "* Uses feeds and fetches to get data into and out of arbitrary operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing computations as graphs\n",
    "=========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simmilarly as happens with Theano, TensorFlow is built with the concept of variable dependence through a graph. \n",
    "* TensorFlow programs are usually structured into a construction phase, that assembles a graph, and an execution phase that uses a session to execute ops in the graph.\n",
    "* TensorFlow can be used from C, C++, and Python programs. It is presently much easier to use the Python library to assemble graphs, as it provides a large set of helper functions not available in the C and C++ libraries.\n",
    "* The session libraries have equivalent functionalities for the three languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Create a Constant op that produces a 1x2 matrix.  The op is\n",
    "# added as a node to the default graph.\n",
    "#\n",
    "# The value returned by the constructor represents the output\n",
    "# of the Constant op.\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "\n",
    "# Create another Constant that produces a 2x1 matrix.\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "\n",
    "# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n",
    "# The returned value, 'product', represents the result of the matrix\n",
    "# multiplication.\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default graph now has three nodes: two constant() ops and one matmul() op. To actually multiply the matrices, and get the result of the multiplication, we must launch the graph in a session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the graph in a session\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated.\n",
    "\n",
    "Launching follows construction. To launch a graph, create a Session object. Without arguments the session constructor launches the default graph.\n",
    "\n",
    "See the  [Session class](https://www.tensorflow.org/versions/master/api_docs/python/client.html#session-management \"Session\")  for the complete session API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "#Allocating just the GPU memory that we need\n",
    "#http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "# Launch the default graph.\n",
    "sess = tf.Session()\n",
    "\n",
    "# To run the matmul op we call the session 'run()' method, passing 'product'\n",
    "# which represents the output of the matmul op.  This indicates to the call\n",
    "# that we want to get the output of the matmul op back.\n",
    "#\n",
    "# All inputs needed by the op are run automatically by the session.  They\n",
    "# typically are run in parallel.\n",
    "#\n",
    "# The call 'run(product)' thus causes the execution of threes ops in the\n",
    "# graph: the two constants and matmul.\n",
    "#\n",
    "# The output of the op is returned in 'result' as a numpy `ndarray` object.\n",
    "result = sess.run(product)\n",
    "print(result)\n",
    "# ==> If everithing goes well we should see [[ 12.]]\n",
    "\n",
    "# Close the Session when we're done.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of use in interactive Python environments, such as IPython you can instead use the InteractiveSession class, and the Tensor.eval() and Operation.run() methods. This avoids having to keep a variable holding the session.\n",
    "\n",
    "The only difference with a regular Session is that an InteractiveSession installs itself as the default session on construction. The methods Tensor.eval() and Operation.run() will use that session to run ops.\n",
    "\n",
    "This is convenient in interactive shells and IPython notebooks, as it avoids having to pass an explicit Session object to run ops.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. -1.]\n"
     ]
    }
   ],
   "source": [
    "# Enter an interactive TensorFlow Session.\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.Variable([1.0, 2.0])\n",
    "a = tf.constant([3.0, 3.0])\n",
    "\n",
    "# Initialize 'x' using the run() method of its initializer op.\n",
    "x.initializer.run()\n",
    "\n",
    "# Add an op to subtract 'a' from 'x'.  Run it and print the result\n",
    "sub = tf.sub(x, a)\n",
    "print(sub.eval())\n",
    "# ==> [-2. -1.]\n",
    "\n",
    "# Close the Session when we're done.\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors and Variables\n",
    "===============================\n",
    "\n",
    "* *TensorFlow* programs use a <b>Tensor</b> data structure to represent all data -- only tensors are passed between operations in the computation graph. You can think of a TensorFlow tensor as an n-dimensional array or list. A tensor has a static type, a rank, and a shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Variable is a value that lives in TensorFlow's computation graph. It can be used and even modified by the computation. In machine learning applications, one generally has the model parameters be Variables.\n",
    "\n",
    "<b>Variables</b> maintain state across executions of the graph. The following example shows a variable serving as a simple counter. See the <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#variables-2\">Variable</a> class for more details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Create a Variable, that will be initialized to the scalar value 0.\n",
    "state = tf.Variable(0, name=\"counter\")\n",
    "\n",
    "# Create an Op to add one to `state`.\n",
    "\n",
    "one = tf.constant(1)\n",
    "new_value = tf.add(state, one)\n",
    "update = tf.assign(state, new_value)\n",
    "#state = new_value\n",
    "\n",
    "# Variables must be initialized by running an `init` Op after having\n",
    "# launched the graph.  We first have to add the `init` Op to the graph.\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph and run the ops.\n",
    "with tf.Session() as sess:\n",
    "  # Run the 'init' op\n",
    "  sess.run(init_op)\n",
    "  # Print the initial value of 'state'\n",
    "  print(sess.run(state))\n",
    "  # Run the op that updates 'state' and print 'state'.\n",
    "  for _ in range(3):\n",
    "    sess.run(update)\n",
    "    print(sess.run(state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches and Feeds \n",
    "=============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "* The examples above introduce tensors into the computation graph by storing them in Constants and Variables. TensorFlow also provides a feed and fetch mechanisms for patching/retrieving a tensor directly into any operation in the graph.\n",
    "\n",
    "* A feed temporarily replaces the output of an operation with a tensor value. **You supply feed data as an argument to a run() call**. The feed is only used for the run call to which it is passed. The most common use case involves designating specific operations to be \"feed\" operations by using tf.placeholder() to create them\n",
    "* To fetch the outputs of operations, execute the graph with a run() call on the Session object and **pass in the tensors to retrieve**. In the previous example we fetched the single node state, but you can also fetch multiple tensors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "#Let's fetch something\n",
    "input1 = tf.constant(3.0)\n",
    "input2 = tf.constant(2.0)\n",
    "input3 = tf.constant(5.0)\n",
    "intermed = tf.add(input2, input3)\n",
    "mul = tf.mul(input1, intermed)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  result = sess.run([mul, intermed])\n",
    "  print(result)\n",
    "\n",
    "# output:\n",
    "# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Lets feed something\n",
    "#A feed temporarily replaces the output of an operation with a tensor value. You supply feed data as an argument to a run() call. \n",
    "#The feed is only used for the run call to which it is passed. The most common use case involves designating specific operations to be \"feed\" operations by using tf.placeholder() to create them:\n",
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.mul(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))\n",
    "\n",
    "# output:\n",
    "# [array([ 14.], dtype=float32)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Usage example: Fitting a line \n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from: https://www.tensorflow.org/versions/master/get_started/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(\"float32\")\n",
    "y_data = x_data * 0.1 + 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but Tensorflow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, array([-0.38865268], dtype=float32), array([ 0.81302577], dtype=float32))\n",
      "(20, array([-0.04451281], dtype=float32), array([ 0.3799592], dtype=float32))\n",
      "(40, array([ 0.0646899], dtype=float32), array([ 0.31953716], dtype=float32))\n",
      "(60, array([ 0.09137238], dtype=float32), array([ 0.30477369], dtype=float32))\n",
      "(80, array([ 0.09789193], dtype=float32), array([ 0.30116642], dtype=float32))\n",
      "(100, array([ 0.09948491], dtype=float32), array([ 0.30028501], dtype=float32))\n",
      "(120, array([ 0.09987415], dtype=float32), array([ 0.30006963], dtype=float32))\n",
      "(140, array([ 0.09996927], dtype=float32), array([ 0.30001703], dtype=float32))\n",
      "(160, array([ 0.0999925], dtype=float32), array([ 0.30000415], dtype=float32))\n",
      "(180, array([ 0.09999818], dtype=float32), array([ 0.30000103], dtype=float32))\n",
      "(200, array([ 0.09999955], dtype=float32), array([ 0.30000025], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph.\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Fit the line.\n",
    "for step in xrange(201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print(step, sess.run(W), sess.run(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Multiple GPU's\n",
    "========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to run TensorFlow on multiple GPUs, you can construct your model in a multi-tower fashion where each tower is assigned to a different GPU. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  44.   56.]\n",
      " [  98.  128.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creates a graph.\n",
    "c = []\n",
    "for d in ['/gpu:0', '/cpu:0']: # It was /gpu:2 but in HAL we have just 1 Titan X\n",
    "  with tf.device(d):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3])\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2])\n",
    "    c.append(tf.matmul(a, b))\n",
    "with tf.device('/cpu:0'):\n",
    "  sum = tf.add_n(c)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print sess.run(sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence-to-Sequence Models in TensorFlow\n",
    "=============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks can learn to model language, as already discussed in the RNN examples (there is also <a href=\"https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html\">this one</a> of RNN in tensorflow). This raises an interesting question: could we condition the generated words on some input and generate a meaningful response? For example, could we train a neural network to translate from English to French? It turns out that the answer is yes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this example we will be able to build and train such a system end-to-end. We are assuming you have already installed via the pip package, have cloned the tensorflow git repository, and are in the root of the git tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This tutorial references the following files from the models/rnn folder in tensorflow instalation path (mine is on /home/jsotaloram/nlp/tensorflow/tensorflow/models/rnn/)\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>**File**</td> <td>**What's in it?**</td>\n",
    "<tr>\n",
    "        <td>seq2seq.py</td> <td>Library for building sequence-to-sequence models.</td>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>translate/data_utils.py</td> <td>Helper functions for preparing translation data.</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "        <td>translate/seq2seq_model.py</td> <td>Neural translation sequence-to-sequence model.</td>\n",
    "\n",
    "</tr>\n",
    "<tr>\n",
    "        <td>translate/translate.py</td> <td>Binary that trains and runs the translation model.</td>\n",
    "\n",
    "</tr>\n",
    "\n",
    "</tr>\n",
    "    \n",
    "</table>\n",
    "<a href=\"https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html\"> Complete tutorial </a><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1,'/home/jsotaloram/nlp/tensorflow/tensorflow/models/rnn/translate/')\n",
    "import seq2seq_model\n",
    "import data_utils\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin                                                          \n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.platform import gfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now let's define the flags for our parameters\n",
    "tf.app.flags.DEFINE_float(\"learning_rate\", 0.5, \"Learning rate.\")\n",
    "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n",
    "                          \"Learning rate decays by this much.\")\n",
    "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n",
    "                          \"Clip gradients to this norm.\")\n",
    "tf.app.flags.DEFINE_integer(\"batch_size\", 64,\n",
    "                            \"Batch size to use during training.\")\n",
    "tf.app.flags.DEFINE_integer(\"size\", 1024, \"Size of each model layer.\")\n",
    "tf.app.flags.DEFINE_integer(\"num_layers\", 3, \"Number of layers in the model.\")\n",
    "tf.app.flags.DEFINE_integer(\"en_vocab_size\", 40000, \"English vocabulary size.\")\n",
    "tf.app.flags.DEFINE_integer(\"fr_vocab_size\", 40000, \"French vocabulary size.\")\n",
    "tf.app.flags.DEFINE_string(\"data_dir\", \"/home/datasets/datasets1/wmt_dataset/\", \"Data directory\")\n",
    "tf.app.flags.DEFINE_string(\"train_dir\", \"/home/jsotaloram/nlp/notebook/pretrained_model\", \"Training directory.\")\n",
    "tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\n",
    "                            \"Limit on the size of training data (0: no limit).\")\n",
    "tf.app.flags.DEFINE_integer(\"steps_per_checkpoint\", 100,\n",
    "                            \"How many training steps to do per checkpoint.\")\n",
    "tf.app.flags.DEFINE_boolean(\"decode\", True,\n",
    "                            \"Set to True for interactive decoding.\")\n",
    "tf.app.flags.DEFINE_boolean(\"self_test\", False,\n",
    "                            \"Run a self-test if this is set to True.\")\n",
    "FLAGS = tf.app.flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use a number of buckets and pad to the closest one for efficiency.                                                    \n",
    "# See seq2seq_model.Seq2SeqModel for details of how they work.                                                             \n",
    "_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(source_path, target_path, max_size=None):\n",
    "  \"\"\"Read data from source and target files and put into buckets.\n",
    "  Args:\n",
    "    source_path: path to the files with token-ids for the source language.\n",
    "    target_path: path to the file with token-ids for the target language;\n",
    "      it must be aligned with the source file: n-th line contains the desired\n",
    "      output for n-th line from the source_path.\n",
    "    max_size: maximum number of lines to read, all other will be ignored;\n",
    "      if 0 or None, data files will be read completely (no limit).\n",
    "  Returns:\n",
    "    data_set: a list of length len(_buckets); data_set[n] contains a list of\n",
    "      (source, target) pairs read from the provided data files that fit\n",
    "      into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n",
    "      len(target) < _buckets[n][1]; source and target are lists of token-ids.\n",
    "  \"\"\"\n",
    "  data_set = [[] for _ in _buckets]\n",
    "  with gfile.GFile(source_path, mode=\"r\") as source_file:\n",
    "    with gfile.GFile(target_path, mode=\"r\") as target_file:\n",
    "      source, target = source_file.readline(), target_file.readline()\n",
    "      counter = 0\n",
    "      while source and target and (not max_size or counter < max_size):\n",
    "        counter += 1\n",
    "        if counter % 100000 == 0:\n",
    "          print(\"  reading data line %d\" % counter)\n",
    "          sys.stdout.flush()\n",
    "        source_ids = [int(x) for x in source.split()]\n",
    "        target_ids = [int(x) for x in target.split()]\n",
    "        target_ids.append(data_utils.EOS_ID)\n",
    "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "            data_set[bucket_id].append([source_ids, target_ids])\n",
    "            break\n",
    "        source, target = source_file.readline(), target_file.readline()\n",
    "  return data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(session, forward_only):\n",
    "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "  model = seq2seq_model.Seq2SeqModel(\n",
    "      FLAGS.en_vocab_size, FLAGS.fr_vocab_size, _buckets,\n",
    "      FLAGS.size, FLAGS.num_layers, FLAGS.max_gradient_norm, FLAGS.batch_size,\n",
    "      FLAGS.learning_rate, FLAGS.learning_rate_decay_factor,\n",
    "      forward_only=forward_only)\n",
    "  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n",
    "  if ckpt and gfile.Exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "  else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    session.run(tf.initialize_all_variables())\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  \"\"\"Train a en->fr translation model using WMT data.\"\"\"\n",
    "  # Prepare WMT data.\n",
    "  print(\"Preparing WMT data in %s\" % FLAGS.data_dir)\n",
    "  en_train, fr_train, en_dev, fr_dev, _, _ = data_utils.prepare_wmt_data(\n",
    "      FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size)\n",
    "  with tf.Session() as sess:\n",
    "    # Create model.\n",
    "    print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n",
    "    model = create_model(sess, False)\n",
    "    # Read data into buckets and compute their sizes.\n",
    "    print (\"Reading development and training data (limit: %d).\"\n",
    "           % FLAGS.max_train_data_size)\n",
    "    dev_set = read_data(en_dev, fr_dev)\n",
    "    train_set = read_data(en_train, fr_train, FLAGS.max_train_data_size)\n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "    # A bucket scale is a list of increasing numbers from 0 to 1 that we'll use\n",
    "    # to select a bucket. Length of [scale[i], scale[i+1]] is proportional to\n",
    "    # the size if i-th training bucket, as used later.\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    while True:\n",
    "      # Choose a bucket according to data distribution. We pick a random number\n",
    "      # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "      random_number_01 = np.random.random_sample()\n",
    "      bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                       if train_buckets_scale[i] > random_number_01])\n",
    "      # Get a batch and make a step.\n",
    "      start_time = time.time()\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          train_set, bucket_id)\n",
    "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                   target_weights, bucket_id, False)\n",
    "      step_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\n",
    "      loss += step_loss / FLAGS.steps_per_checkpoint\n",
    "      current_step += 1\n",
    "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "      if current_step % FLAGS.steps_per_checkpoint == 0:\n",
    "        # Print statistics for the previous epoch.\n",
    "        perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "        print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "               \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                         step_time, perplexity))\n",
    "        # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "        if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "          sess.run(model.learning_rate_decay_op)\n",
    "        previous_losses.append(loss)\n",
    "        # Save checkpoint and zero timer and loss.\n",
    "        checkpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")\n",
    "        model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        # Run evals on development set and print their perplexity.\n",
    "        for bucket_id in xrange(len(_buckets)):\n",
    "          encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "              dev_set, bucket_id)\n",
    "          _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "          eval_ppx = math.exp(eval_loss) if eval_loss < 300 else float('inf')\n",
    "          print(\"  eval: bucket %d perplexity %.2f\" % (bucket_id, eval_ppx))\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode():\n",
    "  with tf.Session() as sess:\n",
    "    # Create model and load parameters.\n",
    "    model = create_model(sess, True)\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "    # Load vocabularies.\n",
    "    en_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.en\" % FLAGS.en_vocab_size)\n",
    "    fr_vocab_path = os.path.join(FLAGS.data_dir,\n",
    "                                 \"vocab%d.fr\" % FLAGS.fr_vocab_size)\n",
    "    en_vocab, _ = data_utils.initialize_vocabulary(en_vocab_path)\n",
    "    _, rev_fr_vocab = data_utils.initialize_vocabulary(fr_vocab_path)\n",
    "    # Decode from standard input.\n",
    "    sys.stdout.write(\"> \")\n",
    "    sys.stdout.flush()\n",
    "    sentence = sys.stdin.readline()\n",
    "    while sentence:\n",
    "      # Get token-ids for the input sentence.\n",
    "      token_ids = data_utils.sentence_to_token_ids(sentence, en_vocab)\n",
    "      # Which bucket does it belong to?\n",
    "      bucket_id = min([b for b in xrange(len(_buckets))\n",
    "                       if _buckets[b][0] > len(token_ids)])\n",
    "      # Get a 1-element batch to feed the sentence to the model.\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "      # Get output logits for the sentence.\n",
    "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
    "                                       target_weights, bucket_id, True)\n",
    "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "      # If there is an EOS symbol in outputs, cut them at that point.\n",
    "      if data_utils.EOS_ID in outputs:\n",
    "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
    "      # Print out French sentence corresponding to outputs.\n",
    "      print(\" \".join([rev_fr_vocab[output] for output in outputs]))\n",
    "      print(\"> \", end=\"\")\n",
    "      sys.stdout.flush()\n",
    "      sentence = sys.stdin.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def self_test():\n",
    "  \"\"\"Test the translation model.\"\"\"\n",
    "  with tf.Session() as sess:\n",
    "    print(\"Self-test for neural translation model.\")\n",
    "    # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "    model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
    "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "    data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
    "                [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "    for _ in xrange(5):  # Train the fake model for 5 steps.\n",
    "      bucket_id = random.choice([0, 1])\n",
    "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "          data_set, bucket_id)\n",
    "      model.step(sess, encoder_inputs, decoder_inputs, target_weights,\n",
    "                 bucket_id, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-test for neural translation model.\n"
     ]
    }
   ],
   "source": [
    "self_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  if FLAGS.self_test:\n",
    "    self_test()\n",
    "  elif FLAGS.decode:\n",
    "    decode()\n",
    "  else:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsotaloram/nlp/tensorflow/tensorflow/models/rnn/translate\n"
     ]
    }
   ],
   "source": [
    "cd /home/jsotaloram/nlp/tensorflow/tensorflow/models/rnn/translate/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --learning_rate: conflicting option string(s): --learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/jsotaloram/nlp/tensorflow/tensorflow/models/rnn/translate/translate.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"learning_rate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Learning rate.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n\u001b[0;32m     58\u001b[0m                           \"Learning rate decays by this much.\")\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/tensorflow/python/platform/default/_flags.pyc\u001b[0m in \u001b[0;36mDEFINE_float\u001b[1;34m(flag_name, default_value, docstring)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mdocstring\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhelpful\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mexplaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m   \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m   \u001b[0m_define_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/tensorflow/python/platform/default/_flags.pyc\u001b[0m in \u001b[0;36m_define_helper\u001b[1;34m(flag_name, default_value, docstring, flagtype)\u001b[0m\n\u001b[0;32m     39\u001b[0m                               \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                               \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                               type=flagtype)\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36madd_argument\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%r is not callable'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1698\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1700\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1701\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[1;31m# resolve any conflicts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \u001b[1;31m# add to actions list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_check_conflict\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m             \u001b[0mconflict_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[1;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                      \u001b[1;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                      in conflicting_actions])\n\u001b[1;32m-> 1483\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument --learning_rate: conflicting option string(s): --learning_rate"
     ]
    }
   ],
   "source": [
    "%run translate.py --data_dir /home/datasets/datasets1/wmt_dataset/ --train_dir /home/jsotaloram/nlp/notebook/pretrained_model --en_vocab_size=40000 --fr_vocab_size=40000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --learning_rate: conflicting option string(s): --learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/home/jsotaloram/nlp/tensorflow/tensorflow/models/rnn/translate/translate.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"learning_rate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Learning rate.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.99,\n\u001b[0;32m     58\u001b[0m                           \"Learning rate decays by this much.\")\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/tensorflow/python/platform/default/_flags.pyc\u001b[0m in \u001b[0;36mDEFINE_float\u001b[1;34m(flag_name, default_value, docstring)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mdocstring\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhelpful\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mexplaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m   \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m   \u001b[0m_define_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/tensorflow/python/platform/default/_flags.pyc\u001b[0m in \u001b[0;36m_define_helper\u001b[1;34m(flag_name, default_value, docstring, flagtype)\u001b[0m\n\u001b[0;32m     39\u001b[0m                               \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                               \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                               type=flagtype)\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36madd_argument\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%r is not callable'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1698\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1700\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1701\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[1;31m# resolve any conflicts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \u001b[1;31m# add to actions list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_check_conflict\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m             \u001b[0mconflict_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[1;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                      \u001b[1;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                      in conflicting_actions])\n\u001b[1;32m-> 1483\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument --learning_rate: conflicting option string(s): --learning_rate"
     ]
    }
   ],
   "source": [
    "%run translate.py --data_dir /home/datasets/datasets1/wmt_dataset/ --train_dir /home/jsotaloram/nlp/notebook/pretrained_model --en_vocab_size=40000 --fr_vocab_size=40000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard: Visualizing Learning\n",
    "==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow programs, we've included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it. \n",
    "We'll run the MNIST tutorial with modifications to see some metrics on tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsotaloram/nlp/tensorflow/tensorflow/examples/tutorials/mnist\n"
     ]
    }
   ],
   "source": [
    "cd /home/jsotaloram/nlp/tensorflow/tensorflow/examples/tutorials/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All the model is this line\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To implement cross-entropy we need to first add a new placeholder to input the correct answers:\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9029\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now lets modify the code to put some summaries in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow.python.platform\n",
    "import input_data\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "argument --learning_rate: conflicting option string(s): --learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-6809d75452f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                      'for unit testing.')\n\u001b[0;32m      6\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'max_steps'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Number of steps to run trainer.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Initial learning rate.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/tensorflow/python/platform/default/_flags.pyc\u001b[0m in \u001b[0;36mDEFINE_float\u001b[1;34m(flag_name, default_value, docstring)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mdocstring\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mhelpful\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mexplaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0muse\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m   \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m   \u001b[0m_define_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/tensorflow/python/platform/default/_flags.pyc\u001b[0m in \u001b[0;36m_define_helper\u001b[1;34m(flag_name, default_value, docstring, flagtype)\u001b[0m\n\u001b[0;32m     39\u001b[0m                               \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                               \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                               type=flagtype)\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36madd_argument\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%r is not callable'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_argument_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1698\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption_strings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1700\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1701\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_positionals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ArgumentGroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_group_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_add_action\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[1;31m# resolve any conflicts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1338\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_conflict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \u001b[1;31m# add to actions list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_check_conflict\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m   1474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m             \u001b[0mconflict_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m             \u001b[0mconflict_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfl_optionals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/argparse-1.3.0-py2.7.egg/argparse.pyc\u001b[0m in \u001b[0;36m_handle_conflict_error\u001b[1;34m(self, action, conflicting_actions)\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                      \u001b[1;32mfor\u001b[0m \u001b[0moption_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                      in conflicting_actions])\n\u001b[1;32m-> 1483\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mArgumentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mconflict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_conflict_resolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconflicting_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mArgumentError\u001b[0m: argument --learning_rate: conflicting option string(s): --learning_rate"
     ]
    }
   ],
   "source": [
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_boolean('fake_data', False, 'If true, uses fake data '\n",
    "                     'for unit testing.')\n",
    "flags.DEFINE_integer('max_steps', 1000, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder('float', [None, 784], name='x-input')\n",
    "W = tf.Variable(tf.zeros([784, 10]), name='weights')\n",
    "b = tf.Variable(tf.zeros([10], name='bias'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a name scope to organize nodes in the graph visualizer\n",
    "with tf.name_scope('Wx_b'):\n",
    "  y = tf.nn.softmax(tf.matmul(x, W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add summary ops to collect data\n",
    "_ = tf.histogram_summary('weights', W)\n",
    "_ = tf.histogram_summary('biases', b)\n",
    "_ = tf.histogram_summary('y', y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder('float', [None, 10], name='y-input')\n",
    "# More name scopes will clean up the graph representation\n",
    "with tf.name_scope('xent'):\n",
    "    cross_entropy = -tf.reduce_sum(y_ * tf.log(y))\n",
    "    _ = tf.scalar_summary('cross entropy', cross_entropy)\n",
    "\n",
    "with tf.name_scope('test'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
    "    _ = tf.scalar_summary('accuracy', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge all the summaries and write them out to /tmp/mnist_logs\n",
    "merged = tf.merge_all_summaries()\n",
    "writer = tf.train.SummaryWriter('/tmp/mnist_logs', sess.graph_def)\n",
    "tf.initialize_all_variables().run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--fake_data FAKE_DATA] [--nofake_data]\n",
      "                   [--max_steps MAX_STEPS] [--learning_rate LEARNING_RATE]\n",
      "__main__.py: error: unrecognized arguments: -f /home/jsotaloram/.ipython/profile_default/security/kernel-1783fdc4-3ea1-43ad-adee-0abf8d6d4ad5.json --profile-dir /home/jsotaloram/.ipython/profile_default\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "  # Train the model, and feed in test data and record summaries every 10 steps\n",
    "for i in range(FLAGS.max_steps):\n",
    "    if i % 10 == 0:  # Record summary data, and the accuracy\n",
    "        if FLAGS.fake_data:\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n",
    "            feed = {x: batch_xs, y_: batch_ys}\n",
    "        else:\n",
    "            feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "        result = sess.run([merged, accuracy], feed_dict=feed)\n",
    "        summary_str = result[0]\n",
    "        acc = result[1]\n",
    "        writer.add_summary(summary_str, i)\n",
    "        print('Accuracy at step %s: %s' % (i, acc))\n",
    "    else:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n",
    "        feed = {x: batch_xs, y_: batch_ys}\n",
    "        sess.run(train_step, feed_dict=feed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IOError [Errno 2] No such file or directory: '/home/jsotaloram/nlp/tensorflow/tensorflow/examples/tutorials/mnist/tensorflow/tensorboard/TAG' on path /home/jsotaloram/nlp/tensorflow/tensorflow/examples/tutorials/mnist/tensorflow/tensorboard/TAG\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jsotaloram/nlp/tensorflow/tensorflow/tensorboard/tensorboard.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m   \u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/anaconda/envs/mindlabenv/lib/python2.7/site-packages/tensorflow/python/platform/default/_app.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_flags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mmain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/jsotaloram/nlp/tensorflow/tensorflow/tensorboard/tensorboard.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(unused_argv)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresource_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_resource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tensorboard/TAG'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TensorBoard is tag: %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "#The command to run tensorflow is \n",
    "%run /home/jsotaloram/nlp/tensorflow/tensorflow/tensorboard/tensorboard.py --logdir=/home/jsotaloram/nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
