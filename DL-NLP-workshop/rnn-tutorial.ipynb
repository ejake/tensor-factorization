{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Tutorial\n",
    "\n",
    "## General applications\n",
    "\n",
    "Natural language processing concerns with the interaction between computer and human natural language. Among different applications, there are:\n",
    " * Translation\n",
    " * Sentiment analysis (e.g. comments on products)\n",
    " * Document categorization (e.g. spam filtering, language identification, news categorization)\n",
    " * Automatic evaluation (e.g. answers to open questions)\n",
    " * Automatic summarization (e.g. explore large collection of documents)\n",
    " * Paraphasing detection (e.g. trend topic analysis on twitter)\n",
    " * Grammar parsing \n",
    " * etc,\n",
    " \n",
    "Standard machine learning approaches can receive directly a set of fixed-length features with numerical or categorical data. However, to deal with sequences such as words or sound, a preprocessing step is usually required (Images adapted from [indico's tutorial](https://indico.io/blog/general-sequence-learning-using-recurrent-neural-nets/)).\n",
    "![How text is dealt](http://i.imgur.com/0aDV3fC.png)\n",
    "However structure is important!:\n",
    "![Structure is important](http://i.imgur.com/xaBWxI2.png)\n",
    "##Language Modeling\n",
    "A language model computes the probability of a sequence of words. The main idea is to learn a model that computes higher probabilities to more likely sequences of words, e.g.:\n",
    "$$P(the\\ cat\\ is\\ small) \\geq  P(small\\ the\\ is\\ cat)$$\n",
    "$$P(walking\\ home\\ after\\ work) \\geq  P(walking\\ house\\ after\\ work)$$\n",
    "\n",
    "In order to get a model like that one can use contextual information of what words are ocurring in similar contexts <img src=\"http://i.imgur.com/j6ryjWW.png\" alt=\"Distributed Space\" height=\"500\" width=\"500\">\n",
    "Above we can see the different contexts in which the word stars is likely to occur. For instance words like constellations and moon are present in the same context of *stars*.\n",
    "\n",
    "Bag of words representation can be extended to deal with sequences through inclusion of bi-grams or n-grams and huge tables of co-occurrence statistics. However, this approach is limited to the number of combinations we can deal with.\n",
    "\n",
    "# Quick review of recurrent neural networks\n",
    "![RNN approach](http://i.imgur.com/6siwjNl.png)\n",
    "RNN models sequences through recurrent connections in the hidden units of the network as depicted above. This structure can be shown in its unfolded version:\n",
    "![RNN approach](http://i.imgur.com/3qvKyoP.png)\n",
    "\n",
    "Recurrent neural networks (RNNs) are a quite popular option to learn a language model and the distributed representation of words. Since they are good at modelling sequences. Theoretically they can condition the model on all previous word on the corpus. The following is the classical architecture of a RNN: <img src=\"http://i.imgur.com/uGNd1LZ.png\">\n",
    "\n",
    "RNNs are also attractive because they are capable of handling an input of arbitrary size. Concretely they have three layers: an input, a hidden and an output layer. Interestingly RNNs combine the input vector with the state vector(hidden layer) using a learned function, then they produce a new state vector and therefore an output vector. Essentially RNNs receive a sequence of vectors as input and produce an output sequence of vectors. **An output produced by a RNN is influenced not only by its current input, but by all the past inputs the RNN has been fed**\n",
    "\n",
    "### A more expressive recurrent unit: Gated Recurrent\n",
    "To overcome issues related to the training process (Vanishing and exploding gradient), hidden units with gates has been proposed. In particular, we will use the Gated Recurrent Unit ([Cho et al.](http://arxiv.org/abs/1409.1259)):\n",
    "\n",
    "<img src=\"gru.png\"/>\n",
    "\n",
    "In summary, $r$ and $z$ works as \"Gates\" that controls the information flow. $r$ (reset gate) allows or blocks the usage from previous states ($h_(t-1)$) in the current state ($h_t$). $z$ (update gate) controls whether $h_t$ is updated.\n",
    "<img src=\"http://img.blog.csdn.net/20150830152611813\"/>\n",
    "\n",
    "#Our practical excercise\n",
    "\n",
    "One can use the learned model to predict the next token iterativelly, so that applying a stochastic process our neural network generates sequences!. We follow architecture tested on [Andrej's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) to learn with a set of different text documents. \n",
    "<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" height=\"500\" width=\"500\">\n",
    "\n",
    "This section helps to build a RNN model using Theano and Blocks frameworks. It requires two python files which include utils function like ploting and monitoring.\n",
    "\n",
    "## Architecture\n",
    "This is the neural network architecture for our char-based NLM:\n",
    "<img src=\"https://raw.githubusercontent.com/fagonzalezo/nn_nlp_tutorial/gh-pages/rnn_architecture.jpg\" width= 400>\n",
    "\n",
    "## Preparing the environment\n",
    "First, we import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import codecs\n",
    "from fuel.datasets import IndexableDataset\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import Mapping\n",
    "from fuel.schemes import ShuffledScheme\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, set the seed to get reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and define parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_length = 50 # number of chars in the sequence\n",
    "embedding_size = 128 # number of hidden units per layer\n",
    "learning_rate = 0.002\n",
    "nepochs = 10 # number of full passes through the training data\n",
    "batch_size = 50 # number of samples taken per each update\n",
    "decay_rate = 0.95 # decay rate for rmsprop\n",
    "step_clipping = 0.5 # clip gradients at this value\n",
    "\n",
    "model_name = 'shakespeare'\n",
    "url_bokeh = 'http://localhost:5006/' # url to online plot training progress\n",
    "text_file = 'input.txt' # input file\n",
    "train_size = 0.95 # fraction of data that goes into train set\n",
    "save_path = 'best_model.pkl' # name to export model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset\n",
    "Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Citizen:\n",
      "Would you proceed especially against Caius Marcius?\n",
      "\n",
      "All:\n",
      "Against him first: he's a very dog to the commonalty.\n",
      "\n",
      "Second Citizen:\n",
      "Consider you what services he has done for his country?\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(text_file, 'r', 'utf-8') as f:\n",
    "    data = f.read()\n",
    "print data[1000:1200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary taking all different characters in the text file and get number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(data) % seq_length > 0:\n",
    "    data = data[:len(data) - len(data) % seq_length + 1]\n",
    "else:\n",
    "    data = data[:len(data) - seq_length + 1]\n",
    "\n",
    "nsamples = len(data) // seq_length\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to go over the dataset taking chunks of characters and transform them into sequences of integers according to the previous dictionary. `targets` are just sequences moved one character to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = numpy.empty((nsamples, seq_length), dtype='uint8')\n",
    "targets = numpy.zeros_like(features)\n",
    "for i, p in enumerate(range(0, len(data) - 1, seq_length)):\n",
    "    features[i] = numpy.array([char_to_ix[ch] for ch in data[p:p + seq_length]])\n",
    "    targets[i] = numpy.array([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now shuffle and split samples into train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build dataset objects\n",
    "nsamples_train = int(nsamples * train_size)\n",
    "train_dataset = IndexableDataset(indexables=OrderedDict(\n",
    "    [('features', features[:nsamples_train]), ('targets', targets[:nsamples_train])]))\n",
    "dev_dataset = IndexableDataset(indexables=OrderedDict(\n",
    "    [('features', features[nsamples_train:]), ('targets', targets[nsamples_train:])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will export build dataset into Fuel streams. To learn more about fuel, check [the docs](http://fuel.readthedocs.org/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transpose_stream(data):\n",
    "    return (data[0].T, data[1].T)\n",
    "\n",
    "# Define the way samples are going to be retrieved\n",
    "train_stream = DataStream(dataset=train_dataset, iteration_scheme=ShuffledScheme(\n",
    "    examples=train_dataset.num_examples, batch_size=batch_size))\n",
    "dev_stream = DataStream(dataset=dev_dataset, iteration_scheme=ShuffledScheme(\n",
    "    examples=dev_dataset.num_examples, batch_size=batch_size))\n",
    "\n",
    "# Required because Recurrent bricks receive as input [sequence, batch, features]\n",
    "train_stream = Mapping(train_stream, transpose_stream)\n",
    "dev_stream = Mapping(dev_stream, transpose_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "Blocks framework helps us to build and train neural networks in an easy manner. Again, we will import required classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import numpy\n",
    "import sys\n",
    "from theano import tensor\n",
    "from blocks import initialization\n",
    "from blocks import roles\n",
    "from blocks.model import Model\n",
    "from blocks.bricks import Linear, NDimensionalSoftmax\n",
    "from blocks.graph import ComputationGraph\n",
    "from blocks.algorithms import StepClipping, GradientDescent, CompositeRule, RMSProp\n",
    "from blocks.extensions import FinishAfter, Timing, Printing, saveload, predicates\n",
    "from blocks.extensions.monitoring import DataStreamMonitoring, TrainingDataMonitoring\n",
    "from blocks.extensions.training import TrackTheBest\n",
    "from blocks.extras.extensions.plot import Plot\n",
    "from blocks.bricks.parallel import Fork\n",
    "from blocks.bricks.recurrent import GatedRecurrent\n",
    "from blocks.bricks.lookup import LookupTable\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.main_loop import MainLoop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using Blocks we are building the RNN architecture. Firstly, we will use 1 lookup table to map from indices of the vocabulary to real N-dimensional vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "x = tensor.imatrix('features')\n",
    "y = tensor.imatrix('targets')\n",
    "\n",
    "lookup = LookupTable(length=vocab_size, dim=embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are adding two RNN layers. In particular we are going to use Gated Recurrent Units ([Cho et al.](http://arxiv.org/abs/1409.1259))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "fork1 = Fork(output_names=['linear1', 'gates1'], name='fork1',\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "grnn1 = GatedRecurrent(dim=embedding_size, name='gru1')\n",
    "\n",
    "# Layer 2\n",
    "fork2 = Fork(output_names=['linear2', 'gates2'], name='fork2',\n",
    "             input_dim=embedding_size, output_dims=[embedding_size, embedding_size * 2])\n",
    "grnn2 = GatedRecurrent(dim=embedding_size, name='gru2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of our model we set a Softmax classifier for each predicted character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax layer\n",
    "hidden_to_output = Linear(name='hidden_to_output', input_dim=embedding_size,\n",
    "                          output_dim=vocab_size)\n",
    "softmax = NDimensionalSoftmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the defined objects, now we are able to build the whole network, performing the forward propagation starting from `x` until `y_hat` prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Propagate x until top brick to get y_hat predictions\n",
    "embedding = lookup.apply(x)\n",
    "linear1, gates1 = fork1.apply(embedding)\n",
    "h1 = grnn1.apply(linear1, gates1)\n",
    "linear2, gates2 = fork2.apply(h1)\n",
    "h2 = grnn2.apply(linear2, gates2)\n",
    "linear_output = hidden_to_output.apply(h2)\n",
    "linear_output.name = 'linear_output'\n",
    "y_hat = softmax.apply(linear_output, extra_ndim=1)\n",
    "y_hat.name = 'y_hat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define our cost function as the cross entropy between predictions (`y_hat`) and original targets (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COST\n",
    "cost = softmax.categorical_cross_entropy(y, linear_output, extra_ndim=1).mean()\n",
    "cost.name = 'cost'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define learning algorithm\n",
    "now, it is required to define initialization strategies for every learnable block. This step allocates variables in GPU memory and sets random values for weights matrices and zeros to biases vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set initialization strategies\n",
    "to_init = [lookup, grnn1, fork1, grnn2, fork2, hidden_to_output]\n",
    "for brick in to_init:\n",
    "    brick.weights_init = initialization.Orthogonal()\n",
    "    brick.biases_init = initialization.Constant(0)\n",
    "    brick.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our algorithm based on the `cost` and the parameters previously defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning algorithm\n",
    "cg = ComputationGraph(cost)\n",
    "step_rules = [RMSProp(learning_rate=learning_rate, decay_rate=decay_rate),\n",
    "              StepClipping(step_clipping)]\n",
    "algorithm = GradientDescent(cost=cost,\n",
    "                            parameters=cg.parameters,\n",
    "                            step_rule=CompositeRule(step_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b, array([65])),\n",
       " (W, array([128,  65])),\n",
       " (initial_state, array([128])),\n",
       " (b, array([128])),\n",
       " (W, array([128, 128])),\n",
       " (initial_state, array([128])),\n",
       " (b, array([128])),\n",
       " (W, array([128, 128])),\n",
       " (b, array([256])),\n",
       " (W, array([128, 256])),\n",
       " (b, array([256])),\n",
       " (W, array([128, 256])),\n",
       " (W, array([ 65, 128])),\n",
       " (state_to_gates, array([128, 256])),\n",
       " (state_to_state, array([128, 128])),\n",
       " (state_to_gates, array([128, 256])),\n",
       " (state_to_state, array([128, 128]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(p, p.shape.eval()) for p in cg.parameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last step. We include some extensions to monitor the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved session configuration for http://localhost:5006/\n",
      "To override, pass 'load_from_config=False' to Session\n"
     ]
    }
   ],
   "source": [
    "# Extensions\n",
    "def track_best(channel, save_path):\n",
    "    tracker = TrackTheBest(channel, choose_best=min)\n",
    "    checkpoint = saveload.Checkpoint(\n",
    "        save_path, after_training=False, use_cpickle=True)\n",
    "    checkpoint.add_condition([\"after_epoch\"],\n",
    "                             predicate=predicates.OnLogRecord('{0}_best_so_far'.format(channel)))\n",
    "    return [tracker, checkpoint]\n",
    "\n",
    "dev_monitor = DataStreamMonitoring(variables=[cost],\n",
    "                                   before_first_epoch=True, after_epoch=True,\n",
    "                                   data_stream=dev_stream, prefix=\"dev\")\n",
    "train_monitor = TrainingDataMonitoring(variables=[cost],\n",
    "                                       before_first_epoch=True,\n",
    "                                       after_batch=True, prefix='tra')\n",
    "\n",
    "extensions = [train_monitor, dev_monitor,\n",
    "    Timing(),\n",
    "    Printing(after_epoch=True),\n",
    "    FinishAfter(after_n_epochs=nepochs),\n",
    "]\n",
    "\n",
    "extensions.extend(track_best('dev_cost', save_path))\n",
    "extensions.append(Plot('rajaquep', server_url='http://localhost:5006/',\n",
    "            channels=[['tra_cost','dev_cost']], before_first_epoch=True, after_batch=False, after_n_batches=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Finally build the main loop and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\t dev_cost: 4.17185317767\n",
      "\t time_initialization: 166.124927044\n",
      "\t tra_cost: nan\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 1\n",
      "\t iterations_done: 424\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 424:\n",
      "\t dev_cost: 1.97672856767\n",
      "\t time_read_data_this_epoch: 0.354751586914\n",
      "\t time_read_data_total: 0.354751586914\n",
      "\t time_train_this_epoch: 737.613509655\n",
      "\t time_train_total: 737.613509655\n",
      "\t tra_cost: 1.92158560597\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.97672856767\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 2\n",
      "\t iterations_done: 848\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 848:\n",
      "\t dev_cost: 1.81351337576\n",
      "\t time_read_data_this_epoch: 0.444695234299\n",
      "\t time_read_data_total: 0.799446821213\n",
      "\t time_train_this_epoch: 772.965376139\n",
      "\t time_train_total: 1510.57888579\n",
      "\t tra_cost: 1.72186559917\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.81351337576\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 3\n",
      "\t iterations_done: 1272\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1272:\n",
      "\t dev_cost: 1.74972575899\n",
      "\t time_read_data_this_epoch: 0.365837574005\n",
      "\t time_read_data_total: 1.16528439522\n",
      "\t time_train_this_epoch: 762.378457069\n",
      "\t time_train_total: 2272.95734286\n",
      "\t tra_cost: 1.66603122107\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.74972575899\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 4\n",
      "\t iterations_done: 1696\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1696:\n",
      "\t dev_cost: 1.69899038853\n",
      "\t time_read_data_this_epoch: 0.41841506958\n",
      "\t time_read_data_total: 1.5836994648\n",
      "\t time_train_this_epoch: 766.270187855\n",
      "\t time_train_total: 3039.22753072\n",
      "\t tra_cost: 1.57014258289\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.69899038853\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 5\n",
      "\t iterations_done: 2120\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2120:\n",
      "\t dev_cost: 1.67616953418\n",
      "\t time_read_data_this_epoch: 0.342025995255\n",
      "\t time_read_data_total: 1.92572546005\n",
      "\t time_train_this_epoch: 755.641776323\n",
      "\t time_train_total: 3794.86930704\n",
      "\t tra_cost: 1.47017474905\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.67616953418\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 6\n",
      "\t iterations_done: 2544\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2544:\n",
      "\t dev_cost: 1.65916594287\n",
      "\t time_read_data_this_epoch: 0.306282043457\n",
      "\t time_read_data_total: 2.23200750351\n",
      "\t time_train_this_epoch: 513.177725554\n",
      "\t time_train_total: 4308.04703259\n",
      "\t tra_cost: 1.45402321082\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.65916594287\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 7\n",
      "\t iterations_done: 2968\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2968:\n",
      "\t dev_cost: 1.64371163717\n",
      "\t time_read_data_this_epoch: 0.260936498642\n",
      "\t time_read_data_total: 2.49294400215\n",
      "\t time_train_this_epoch: 483.379574537\n",
      "\t time_train_total: 4791.42660713\n",
      "\t tra_cost: 1.41756063704\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.64371163717\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 8\n",
      "\t iterations_done: 3392\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3392:\n",
      "\t dev_cost: 1.63377466007\n",
      "\t time_read_data_this_epoch: 0.352968454361\n",
      "\t time_read_data_total: 2.84591245651\n",
      "\t time_train_this_epoch: 477.109249353\n",
      "\t time_train_total: 5268.53585649\n",
      "\t tra_cost: 1.46647894742\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.63377466007\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 9\n",
      "\t iterations_done: 3816\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3816:\n",
      "\t dev_cost: 1.63781615662\n",
      "\t time_read_data_this_epoch: 0.291999578476\n",
      "\t time_read_data_total: 3.13791203499\n",
      "\t time_train_this_epoch: 482.955493927\n",
      "\t time_train_total: 5751.49135041\n",
      "\t tra_cost: 1.40783780247\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "AFTER ANOTHER EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.63377466007\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 10\n",
      "\t iterations_done: 4240\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4240:\n",
      "\t dev_cost: 1.62199376454\n",
      "\t time_read_data_this_epoch: 0.264189481735\n",
      "\t time_read_data_total: 3.40210151672\n",
      "\t time_train_this_epoch: 489.169595003\n",
      "\t time_train_total: 6240.66094542\n",
      "\t tra_cost: 1.41779560198\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "TRAINING HAS BEEN FINISHED:\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t best_dev_cost: 1.62199376454\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: False\n",
      "\t epochs_done: 10\n",
      "\t iterations_done: 4240\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4240:\n",
      "\t dev_cost: 1.62199376454\n",
      "\t dev_cost_best_so_far: True\n",
      "\t saved_to: ('best_model.pkl',)\n",
      "\t time_read_data_this_epoch: 0.264189481735\n",
      "\t time_read_data_total: 3.40210151672\n",
      "\t time_train_this_epoch: 489.169595003\n",
      "\t time_train_total: 6240.66094542\n",
      "\t tra_cost: 1.41779560198\n",
      "\t training_finish_requested: True\n",
      "\t training_finished: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_loop = MainLoop(data_stream=train_stream, algorithm=algorithm,\n",
    "                     model=Model(cost), extensions=extensions)\n",
    "main_loop.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Section                                  Time     % of total\n",
      "------------------------------------------------------------\n",
      "Before training                          0.01          0.00%\n",
      "  TrainingDataMonitoring                 0.00          0.00%\n",
      "  DataStreamMonitoring                   0.00          0.00%\n",
      "  Timing                                 0.01          0.00%\n",
      "  Printing                               0.00          0.00%\n",
      "  FinishAfter                            0.00          0.00%\n",
      "  TrackTheBest                           0.00          0.00%\n",
      "  Checkpoint                             0.00          0.00%\n",
      "  Plot                                   0.00          0.00%\n",
      "  Other                                  0.00          0.00%\n",
      "Initialization                         166.12          2.52%\n",
      "Training                              6415.07         97.48%\n",
      "  Before epoch                          11.26          0.17%\n",
      "    TrainingDataMonitoring               0.00          0.00%\n",
      "    DataStreamMonitoring                11.20          0.17%\n",
      "    Timing                               0.00          0.00%\n",
      "    Printing                             0.00          0.00%\n",
      "    FinishAfter                          0.00          0.00%\n",
      "    TrackTheBest                         0.00          0.00%\n",
      "    Checkpoint                           0.00          0.00%\n",
      "    Plot                                 0.06          0.00%\n",
      "    Other                                0.00          0.00%\n",
      "  Epoch                               6254.41         95.03%\n",
      "    Read data                            3.40          0.05%\n",
      "    Before batch                         3.16          0.05%\n",
      "      TrainingDataMonitoring             0.46          0.01%\n",
      "      DataStreamMonitoring               0.26          0.00%\n",
      "      Timing                             0.22          0.00%\n",
      "      Printing                           0.65          0.01%\n",
      "      FinishAfter                        0.18          0.00%\n",
      "      TrackTheBest                       0.13          0.00%\n",
      "      Checkpoint                         0.14          0.00%\n",
      "      Plot                               0.55          0.01%\n",
      "      Other                              0.58          0.01%\n",
      "    Train                             6240.66         94.83%\n",
      "    After batch                          6.51          0.10%\n",
      "      TrainingDataMonitoring             2.86          0.04%\n",
      "      DataStreamMonitoring               0.44          0.01%\n",
      "      Timing                             0.39          0.01%\n",
      "      Printing                           0.75          0.01%\n",
      "      FinishAfter                        0.18          0.00%\n",
      "      TrackTheBest                       0.18          0.00%\n",
      "      Checkpoint                         0.16          0.00%\n",
      "      Plot                               0.88          0.01%\n",
      "      Other                              0.67          0.01%\n",
      "    Other                                0.68          0.01%\n",
      "  After epoch                          149.34          2.27%\n",
      "    TrainingDataMonitoring               0.00          0.00%\n",
      "    DataStreamMonitoring                92.39          1.40%\n",
      "    Timing                               0.00          0.00%\n",
      "    Printing                             0.04          0.00%\n",
      "    FinishAfter                          0.00          0.00%\n",
      "    TrackTheBest                         0.00          0.00%\n",
      "    Checkpoint                          56.91          0.86%\n",
      "    Plot                                 0.00          0.00%\n",
      "    Other                                0.00          0.00%\n",
      "  Other                                  0.05          0.00%\n",
      "After training                           0.01          0.00%\n",
      "  TrainingDataMonitoring                 0.00          0.00%\n",
      "  DataStreamMonitoring                   0.00          0.00%\n",
      "  Timing                                 0.00          0.00%\n",
      "  Printing                               0.01          0.00%\n",
      "  FinishAfter                            0.00          0.00%\n",
      "  TrackTheBest                           0.00          0.00%\n",
      "  Checkpoint                             0.00          0.00%\n",
      "  Plot                                   0.00          0.00%\n",
      "  Other                                  0.00          0.00%\n"
     ]
    }
   ],
   "source": [
    "main_loop.profile.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text\n",
    "Hopefully, Our model is now good to predict the next character given a sequence. Thus, we can use it to generate text by feed the model with its own output iterativelly. We first define a function, so we can easily play with the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample(x_curr, predict, temperature=1.0):\n",
    "    '''\n",
    "    Propagate x_curr in the sequence and sample next element according to\n",
    "    temperature sampling.\n",
    "    Return: sample element and an array of hidden states produced by fprop.\n",
    "    '''\n",
    "    hiddens = predict(x_curr)\n",
    "    probs = hiddens.pop()\n",
    "    #Get prob. distribution of the last element in the last seq of the batch\n",
    "    probs = probs[-1,-1,:].astype('float64')\n",
    "    if numpy.random.binomial(1, temperature) == 1:\n",
    "        probs = probs / probs.sum()\n",
    "        sample = numpy.random.multinomial(1, probs).nonzero()[0][0]\n",
    "    else:\n",
    "        sample = probs.argmax()\n",
    "\n",
    "    return sample, hiddens\n",
    "\n",
    "hiddens = []\n",
    "initials = []\n",
    "for brick in [grnn1, grnn2]:\n",
    "    hiddens.extend(VariableFilter(theano_name=brick.name+'_apply_states')(main_loop.model.variables))\n",
    "    initials.extend(VariableFilter(roles=[roles.INITIAL_STATE])(brick.parameters))\n",
    "\n",
    "predict = theano.function([x], hiddens + [y_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the initial characters or pick the first one at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "primetext = ix_to_char[numpy.random.randint(vocab_size)]\n",
    "#primetext = 'KING RICHARD'\n",
    "primetext = ''.join([ch for ch in primetext if ch in char_to_ix.keys()])\n",
    "    \n",
    "x_curr = numpy.expand_dims(\n",
    "    numpy.array([char_to_ix[ch] for ch in primetext], dtype='uint8'), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "length = 2000\n",
    "temperature = 0.4\n",
    "for initial in initials:\n",
    "    initial.set_value(numpy.zeros_like(initial.eval()))\n",
    "sys.stdout.write('Starting sampling\\n' + primetext)\n",
    "for _ in range(length):\n",
    "    idx, newinitials = sample(x_curr, predict, temperature)\n",
    "    sys.stdout.write(ix_to_char[idx])\n",
    "    x_curr = [[idx]]\n",
    "    for initial, newinitial in zip(initials, newinitials):\n",
    "       initial.set_value(newinitial[-1].flatten())\n",
    "\n",
    "sys.stdout.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And that concludes the tutorial...\n",
    "<blockquote class=\"twitter-video\" lang=\"en\"><p lang=\"en\" dir=\"ltr\">â€¦ and that concludes Machine Learning 101. Now, go forth and apply what you&#39;ve learned to real data! <a href=\"http://t.co/D6wSKgdjeM\">pic.twitter.com/D6wSKgdjeM</a></p>&mdash; ML Hipster (@ML_Hipster) <a href=\"https://twitter.com/ML_Hipster/status/633954383542128640\">August 19, 2015</a></blockquote>\n",
    "<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "<img src=\"http://i.imgur.com/ZfkhOt4.png\" style=\"max-width:100%; width: 60%; max-width: none; float:left;\"/><img src=\"https://pbs.twimg.com/media/CPhVYYbUkAA-m6D.jpg:small\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
